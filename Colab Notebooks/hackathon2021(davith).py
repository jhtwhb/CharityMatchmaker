# -*- coding: utf-8 -*-
"""Hackathon2021(Davith).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17CeZdm5SwHsccghRYBcITK6K5wsleNRT

# Setting Up PySpark
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
!tar xf spark-3.1.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"

import findspark
import string
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from random import randint
import time
from pyspark.sql import SQLContext
from pyspark.sql import functions as F
spark = SparkSession.builder.appName("Hackathon2021").getOrCreate()

sc = spark.sparkContext

sqlContext = SQLContext(sc)

"""# Kaggle: USA Charities Data"""

# directoryPath to where the csv files for USA Charities Data is 
directoryPath = '/content/drive/MyDrive/HackAThon2021/Data/Kaggle USA Charities Data/*.csv'

# We now have a dataframe df that has all of the entries from all 50 states.
df = sqlContext.read.option("header", "true").load('/content/drive/MyDrive/HackAThon2021/Data/Kaggle USA Charities Data/*.csv', 'csv')

# Count how many entries we have.
numEntries = df.count()
print(numEntries)

# Look at the Schema to look at what fields we are working with
df.printSchema()

# How many Categories do we have 
numCat = df.groupby('Category').count().count()

print(numCat)

df.show()

df.groupby('Telephone').count().show()
# Looks like the telephone sectino of this table is useless.

# Drop the Telephone column
df = df.drop("Telephone")
df.printSchema()

"""# Charity Navigator Scores Expenses Dataset"""

# Load the csv into a dataframe

df2 = sqlContext.read.option("header", "true").load('/content/drive/MyDrive/HackAThon2021/Data/Charity Navigator Scores Expenses Dataset/CLEAN_charity_data.csv', 'csv')

print(df2.count())

df2.groupBy("state").count().show()

# Look at the schema so we know what fields we're working with

df2.printSchema()

"""# EIN for both data sets"""

# We will remove any hyphens in the EIN's so we can join on them

from pyspark.sql.functions import regexp_replace

new_df = df.withColumn('EIN', regexp_replace("EIN", "[^0-9]", ""))

new_df.show()
print(new_df.count())

print(new_df.filter("EIN = '463802710'").count())
# We know that we did it correctly

# Now we do the same thing to the other table
new_df2 = df2.withColumn('ein', regexp_replace("EIN", "[^0-9]", ""))

new_df2.show()

print(new_df2.filter("EIN = '930642086'").count())
# We know we did it correctly

# How many entries do we have in new_df2?
print(new_df2.count())

# Before we join, we want to drop the Name, State, and Category from the first df so we don't have duplicates.

new_df = new_df.drop('Name', 'Category', 'State')

# Join these two on the EIN's
result = new_df.join(new_df2, on=['EIN'], how='inner')

print(result.count())

result.first()

result.printSchema()

#Show the counts of each size of organization. 
result.groupBy(['size']).count().show()

#Write Out this new table to csv's

result.write.option("header", "true").csv("/content/drive/MyDrive/HackAThon2021/Data/CharityNavAndUSACharities")

# Count how many categories we have. We will use this information in

temp = result.groupBy(["category"]).count()
temp.sort(F.col("count").desc()).show()

result.groupBy("state").count().sort(F.col("count").desc()).show()

# Now we will use Spark template table to run SQL queries.

# Create the temp table
result.registerTempTable("resultTemp");

num_of_small_charities = sqlContext.sql("SELECT * FROM resultTemp WHERE size = 'small'").count()

print(num_of_small_charities)

sqlContext.sql("SELECT * FROM resultTemp WHERE size = 'small'").show()



# Query for high transparency charities

avg_ascore = sqlContext.sql("SELECT AVG(ascore) as averge FROM resultTemp WHERE ascore < 100")

avg_ascore_num = avg_ascore.collect()[0][0]

avg_ascore.show()

# Any score above our average is considered a High Transparency Charity

highTrans = sqlContext.sql("SELECT * FROM resultTemp WHERE ascore > " + str(avg_ascore_num))

print(highTrans.count())

highTrans.show()

# Query for low Financial Health charities

avg_fscore = sqlContext.sql("SELECT AVG(fscore) as averge FROM resultTemp WHERE fscore < 100")

avg_fscore_num = avg_fscore.collect()[0][0]

avg_fscore.show()

# Any score below our average is considered a Low Fanicial Health Charity

lowFanHealth = sqlContext.sql("SELECT * FROM resultTemp WHERE fscore < " + str(avg_fscore_num))

print(lowFanHealth.count())

lowFanHealth.show()

# Query for High Rated charities
# Upoon investigation, I found that the some of the States were 86.1 adn 81.68 and were causing and invalid average so they were removed from the average computation. 

avg_score = sqlContext.sql("SELECT AVG(score) as average FROM resultTemp WHERE score < 100" )

avg_score.show()

avg_score_num = avg_score.collect()[0][0]

print(avg_score_num)

# Any score above our average is considered a High Ratedy Charity

highRated = sqlContext.sql("SELECT * FROM resultTemp WHERE score > " + str(avg_score_num))

print(highRated.count())

highRated.show()

# Query for Highly Expense charities

avg_expense = sqlContext.sql("SELECT AVG(tot_exp) as averge FROM resultTemp")

avg_expense.show()

avg_expense_num = avg_expense.collect()[0][0]

print(avg_expense_num)

# Any score above our average is considered a Highly Expense Charity

highExpense = result.filter(result["tot_exp"] > avg_expense_num)

testp = sqlContext.sql("SELECT * FROM resultTemp WHERE tot_exp > " + str(avg_expense_num))

print(highExpense.count())

highExpense.show()
testp.show()
print(testp.count())

result.printSchema()